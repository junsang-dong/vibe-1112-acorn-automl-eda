import os
import pandas as pd
import numpy as np
import json
import base64
import io
from flask import Flask, request, jsonify, render_template, send_from_directory
from werkzeug.utils import secure_filename
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ì´ ì‚¬ìš©
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import xgboost as xgb
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# ì—…ë¡œë“œ í´ë” ìƒì„±
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
@app.route('/uploads/<filename>')
def uploaded_file(filename):
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class DataAnalyzer:
    def __init__(self):
        self.df = None
        self.numeric_cols = []
        self.categorical_cols = []
        self.target_col = None
        self.models = {}
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
    def load_data(self, file_path):
        """CSV íŒŒì¼ ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
        try:
            self.df = pd.read_csv(file_path)
            
            # 100ê°œ ìƒ˜í”Œë§
            if len(self.df) > 100:
                self.df = self.df.sample(n=100, random_state=42).reset_index(drop=True)
            
            # ê¸°ë³¸ ì „ì²˜ë¦¬
            self.df = self.df.dropna()
            
            # ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜
            self.numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
            self.categorical_cols = self.df.select_dtypes(include=['object', 'bool']).columns.tolist()
            
            # íƒ€ê²Ÿ ë³€ìˆ˜ ìë™ ê°ì§€ (Churn, Target, Label ë“±)
            target_candidates = ['Churn', 'Target', 'Label', 'Class', 'churn', 'target', 'label', 'class']
            for col in target_candidates:
                if col in self.df.columns:
                    self.target_col = col
                    break
            
            # íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©
            if self.target_col is None:
                self.target_col = self.df.columns[-1]
            
            return True
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
            for col in self.categorical_cols:
                if col != self.target_col:
                    le = LabelEncoder()
                    self.df[col] = le.fit_transform(self.df[col].astype(str))
                    self.label_encoders[col] = le
            
            # íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì½”ë”©
            if self.df[self.target_col].dtype == 'object':
                le_target = LabelEncoder()
                self.df[self.target_col] = le_target.fit_transform(self.df[self.target_col])
                self.label_encoders[self.target_col] = le_target
            
            return True
        except Exception as e:
            print(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def get_basic_stats(self):
        """ê¸°ë³¸ í†µê³„ ì •ë³´"""
        # float32ë¥¼ float64ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°
        numeric_stats = {}
        if self.numeric_cols:
            desc_stats = self.df[self.numeric_cols].describe()
            for col in desc_stats.columns:
                numeric_stats[col] = {k: float(v) for k, v in desc_stats[col].to_dict().items()}
        
        stats = {
            'shape': self.df.shape,
            'columns': list(self.df.columns),
            'numeric_columns': self.numeric_cols,
            'categorical_columns': self.categorical_cols,
            'target_column': self.target_col,
            'missing_values': {k: int(v) for k, v in self.df.isnull().sum().to_dict().items()},
            'target_distribution': {k: int(v) for k, v in self.df[self.target_col].value_counts().to_dict().items()},
            'numeric_stats': numeric_stats
        }
        return stats
    
    def get_correlation_matrix(self):
        """ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤"""
        if not self.numeric_cols:
            return None
        
        corr_matrix = self.df[self.numeric_cols + [self.target_col]].corr()
        # float32ë¥¼ float64ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°
        return {k: {kk: float(vv) for kk, vv in v.items()} for k, v in corr_matrix.to_dict().items()}
    
    def create_correlation_heatmap(self):
        """ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±"""
        if not self.numeric_cols:
            return None
        
        plt.figure(figsize=(12, 10))
        corr_matrix = self.df[self.numeric_cols + [self.target_col]].corr()
        
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlBu_r', 
                   center=0, square=True, linewidths=0.5)
        plt.title('Correlation Heatmap', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()
        
        return img_base64
    
    def train_models(self):
        """ëª¨ë¸ í•™ìŠµ"""
        try:
            # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
            X = self.df.drop(columns=[self.target_col])
            y = self.df[self.target_col]
            
            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            
            # ìŠ¤ì¼€ì¼ë§
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # ëª¨ë¸ ì •ì˜
            models = {
                'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),
                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5),
                'XGBoost': xgb.XGBClassifier(random_state=42, max_depth=3, n_estimators=100)
            }
            
            results = {}
            
            for name, model in models.items():
                # ëª¨ë¸ í•™ìŠµ
                if name == 'XGBoost':
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y)) == 2 else None
                else:
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if len(np.unique(y)) == 2 else None
                
                # ì„±ëŠ¥ í‰ê°€
                accuracy = accuracy_score(y_test, y_pred)
                
                # êµì°¨ ê²€ì¦
                if name == 'XGBoost':
                    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
                else:
                    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
                
                results[name] = {
                    'accuracy': float(accuracy),
                    'cv_mean': float(cv_scores.mean()),
                    'cv_std': float(cv_scores.std()),
                    'predictions': y_pred.tolist(),
                    'feature_importance': None
                }
                
                # í”¼ì²˜ ì¤‘ìš”ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
                if hasattr(model, 'feature_importances_'):
                    feature_importance = {k: float(v) for k, v in zip(X.columns, model.feature_importances_)}
                    results[name]['feature_importance'] = feature_importance
                
                # AUC (ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš°)
                if y_pred_proba is not None and len(np.unique(y)) == 2:
                    try:
                        auc = roc_auc_score(y_test, y_pred_proba)
                        results[name]['auc'] = float(auc)
                    except:
                        pass
                
                self.models[name] = model
            
            return results
            
        except Exception as e:
            print(f"ëª¨ë¸ í•™ìŠµ ì˜¤ë¥˜: {e}")
            return {}
    
    def create_feature_importance_plot(self, model_name):
        """í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ ìƒì„±"""
        if model_name not in self.models or not hasattr(self.models[model_name], 'feature_importances_'):
            return None
        
        model = self.models[model_name]
        feature_importance = {k: float(v) for k, v in zip(self.df.drop(columns=[self.target_col]).columns, model.feature_importances_)}
        
        # ìƒìœ„ 10ê°œ í”¼ì²˜ë§Œ í‘œì‹œ
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]
        features, importance = zip(*sorted_features)
        
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(features)), importance)
        plt.yticks(range(len(features)), features)
        plt.xlabel('Feature Importance')
        plt.title(f'{model_name} - Feature Importance (Top 10)', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()
        
        return img_base64
    
    def create_target_distribution_plot(self):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ê·¸ë˜í”„"""
        plt.figure(figsize=(10, 6))
        
        target_counts = self.df[self.target_col].value_counts()
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        plt.subplot(1, 2, 1)
        target_counts.plot(kind='bar', color=['#2ecc71', '#e74c3c'])
        plt.title('Target Distribution (Count)', fontweight='bold')
        plt.xlabel('Target Value')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        
        # íŒŒì´ ì°¨íŠ¸
        plt.subplot(1, 2, 2)
        plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', 
                colors=['#2ecc71', '#e74c3c'], startangle=90)
        plt.title('Target Distribution (Percentage)', fontweight='bold')
        
        plt.tight_layout()
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()
        
        return img_base64

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
analyzer = DataAnalyzer()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    """CSV íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 400
        
        if file and file.filename.lower().endswith('.csv'):
            filename = secure_filename(file.filename)
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(file_path)
            
            # ë°ì´í„° ë¡œë“œ
            if not analyzer.load_data(file_path):
                return jsonify({'error': 'ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 400
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            if not analyzer.preprocess_data():
                return jsonify({'error': 'ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 400
            
            # ê¸°ë³¸ í†µê³„
            basic_stats = analyzer.get_basic_stats()
            
            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
            correlation_matrix = analyzer.get_correlation_matrix()
            
            # ì‹œê°í™”
            correlation_heatmap = analyzer.create_correlation_heatmap()
            target_distribution = analyzer.create_target_distribution_plot()
            
            # ëª¨ë¸ í•™ìŠµ
            model_results = analyzer.train_models()
            
            # í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„
            feature_importance_plots = {}
            for model_name in model_results.keys():
                plot = analyzer.create_feature_importance_plot(model_name)
                if plot:
                    feature_importance_plots[model_name] = plot
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                'success': True,
                'basic_stats': basic_stats,
                'correlation_matrix': correlation_matrix,
                'correlation_heatmap': correlation_heatmap,
                'target_distribution': target_distribution,
                'model_results': model_results,
                'feature_importance_plots': feature_importance_plots
            }
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(file_path)
            
            return jsonify(result)
        
        else:
            return jsonify({'error': 'CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'}), 400
            
    except Exception as e:
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/predict', methods=['POST'])
def predict():
    """ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡"""
    try:
        data = request.get_json()
        
        if not data or 'features' not in data:
            return jsonify({'error': 'ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        features = data['features']
        model_name = data.get('model', 'Random Forest')
        
        if model_name not in analyzer.models:
            return jsonify({'error': 'ì„ íƒëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df_predict = pd.DataFrame([features])
        
        # ì „ì²˜ë¦¬ (ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©)
        for col in analyzer.categorical_cols:
            if col != analyzer.target_col and col in df_predict.columns:
                if col in analyzer.label_encoders:
                    try:
                        df_predict[col] = analyzer.label_encoders[col].transform(df_predict[col].astype(str))
                    except:
                        # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš° ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´
                        df_predict[col] = 0
        
        # ì˜ˆì¸¡
        model = analyzer.models[model_name]
        
        if model_name == 'XGBoost':
            prediction = model.predict(df_predict)[0]
            prediction_proba = model.predict_proba(df_predict)[0] if hasattr(model, 'predict_proba') else None
        else:
            df_predict_scaled = analyzer.scaler.transform(df_predict)
            prediction = model.predict(df_predict_scaled)[0]
            prediction_proba = model.predict_proba(df_predict_scaled)[0] if hasattr(model, 'predict_proba') else None
        
        result = {
            'prediction': int(prediction),
            'prediction_proba': [float(p) for p in prediction_proba.tolist()] if prediction_proba is not None else None
        }
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': f'ì˜ˆì¸¡ ì˜¤ë¥˜: {str(e)}'}), 500

if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ AutoML ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ ì‹œì‘")
    print("=" * 60)
    print("ğŸ“Š ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080 ì ‘ì†")
    print("ğŸ“ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìë™ ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    app.run(debug=True, host='127.0.0.1', port=8080)
